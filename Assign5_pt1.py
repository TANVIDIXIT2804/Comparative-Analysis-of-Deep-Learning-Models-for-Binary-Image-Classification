# -*- coding: utf-8 -*-
"""Copy of Ass5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UlFZri4pUGh9OwsRlMzFgSTPeGe_QABQ
"""

pip install tensorflow

# First Section: Importing Libraries
import os
import requests
from bs4 import BeautifulSoup

# Second Section: Declare important variables
google_image = "https://www.google.com/search?site=&tbm=isch&source=hp&biw=1873&bih=36990&"

user_agent = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537."
}

# Third Section: Build the main function
saved_folder = 'images_t'


def main():
    if not os.path.exists(saved_folder):
        os.mkdir(saved_folder)
    download_images()


# Fourth Section: Build the download function
# def download_images():
#     data = input('What are you looking for? ')
#     n_images = int(input('How many images do you want? '))

#     print('searching...')

#     search_url = google_image + 'q=' + data

#     response = requests.get(search_url, headers=user_agent)

#     html = response.text

#     soup = BeautifulSoup(html, 'html.parser')

#     results = soup.findAll('img', {'class': 'rg_i Q4LuWd'})

#     count = 1
#     links = []
#     for result in results:
#         try:
#             link = result['data-src']
#             links.append(link)
#             count += 1
#             if(count > n_images):
#                 break

#         except KeyError:
#             continue

#     print(f"Downloading {len(links)} images...")

#     for i, link in enumerate(links):
#         response = requests.get(link)

#         image_name = saved_folder + '/' + data + str(i+1) + '.jpg'

#         with open(image_name, 'wb') as fh:
#             fh.write(response.content)

def download_images():
    data = input('What are you looking for? ')
    n_images = int(input('How many images do you want? '))

    print('searching...')

    links = []
    count = 0
    start = 0
    while count < n_images:
        # construct the search URL
        search_url = google_image + 'q=' + data + '&start=' + str(start)

        # send the request and parse the response
        response = requests.get(search_url, headers=user_agent)
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')

        # extract the image links from the HTML
        results = soup.findAll('img', {'class': 'rg_i Q4LuWd'})
        for result in results:
            try:
                link = result['data-src']
                links.append(link)
                count += 1
                if count >= n_images:
                    break
            except KeyError:
                continue

        # extract the ijn token from the HTML for the next page
        scripts = soup.findAll('script')
        for script in scripts:
            if 'data-ijt="' in str(script):
                ijn = str(script).split('data-ijt="')[1].split('"')[0]
                start += 20
                break

    print(f"Downloading {len(links)} images...")

    # download the images
    for i, link in enumerate(links):
        response = requests.get(link)

        image_name = saved_folder + '/' + data + str(i+1) + '.jpg'

        with open(image_name, 'wb') as fh:
            fh.write(response.content)



# Fifth Section: Run your code
if __name__ == "__main__":
    main()

!zip -r /content/images.zip /content/images_e

!zip -r /content/images.zip /content/images_t

pip install split-folders

import splitfolders

image_directory= r'/content/all'
splitfolders.ratio(image_directory, output="output",
        seed=42, ratio=(0.8, 0, 0.2), group_prefix=None, move=False) # default values

# plot some images from Elsa training

from matplotlib import pyplot
from matplotlib.image import imread
# define location of dataset
folder = '/content/output/train/images_e'
# plot first few images
for i in range(1,10):
 # define subplot
 pyplot.subplot(3,3, int(i))
 # define filename
 filename = folder + '/' + 'elsa' + str(i) + '.jpg'
 # load image pixels
 image = imread(filename)
 # plot raw pixel data
 pyplot.imshow(image)
# show the figure
pyplot.show()

# plot some images from Tinkerbell training

from matplotlib import pyplot
from matplotlib.image import imread
# define location of dataset
folder = '/content/output/train/images_t'
# plot first few images
for i in range(1,10):
 # define subplot
 pyplot.subplot(3,3, int(i))
 # define filename
 filename = folder + '/' + 'tinkerbell' + str(i) + '.jpg'
 # load image pixels
 image = imread(filename)
 # plot raw pixel data
 pyplot.imshow(image)
# show the figure
pyplot.show()

!unzip out.zip

"""#VGG Block 1"""

import sys
from matplotlib import pyplot
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator
import time
 
# define cnn model
def VGG_1():
 model = Sequential()
 model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)))
 model.add(MaxPooling2D((2, 2)))
 model.add(Flatten())
 model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
 model.add(Dense(1, activation='sigmoid'))
 # compile model
 opt = SGD(lr=0.001, momentum=0.9)
 model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])
 print(model.summary())
 return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
 # plot loss
 pyplot.subplot(211)
 pyplot.title('VGG Block 1 - Cross Entropy Loss')
 pyplot.plot(history.history['loss'], color='blue', label='train')
 pyplot.plot(history.history['val_loss'], color='orange', label='test')
 # plot accuracy
 pyplot.subplot(212)
 pyplot.title('VGG Block 1 - Classification Accuracy')
 pyplot.plot(history.history['accuracy'], color='blue', label='train')
 pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
 pyplot.show()
 
# run the test harness for evaluating a model
def run_test_harness():
 # define model
 model = VGG_1()
 # create data generator
 datagen = ImageDataGenerator(rescale=1.0/255.0)
 # prepare iterators
 train_it = datagen.flow_from_directory('/content/output/train',
 class_mode='binary', batch_size=64, target_size=(200, 200))
 test_it = datagen.flow_from_directory('/content/output/test',
 class_mode='binary', batch_size=64, target_size=(200, 200))

 t0 = time.time()
 # fit model
 history = model.fit(train_it, steps_per_epoch=len(train_it),
 validation_data=test_it, validation_steps=len(test_it), epochs=20, verbose=0)
 print("Training time:", time.time()-t0)

 predict = model.predict(test_it, steps=len(test_it))

 # evaluate model
 _, acc = model.evaluate(test_it, steps=len(test_it), verbose=0)
 print('> %.3f' % (acc * 100.0))

 _, acc_t = model.evaluate(train_it, steps=len(train_it), verbose=0)
 print('> %.3f' % (acc_t * 100.0))

 print(history.history['loss'])
#  print('Training Loss: %.3f' % train_loss)



 # learning curves
 summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()

"""#VGG Block 3"""

import sys
from matplotlib import pyplot
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator
import time

def VGG_3():
 model = Sequential()
 model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)))
 model.add(MaxPooling2D((2, 2)))
 model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
 model.add(MaxPooling2D((2, 2)))
 model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
 model.add(MaxPooling2D((2, 2)))
 model.add(Flatten())
 model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
 model.add(Dense(1, activation='sigmoid'))
 # compile model
 opt = SGD(lr=0.001, momentum=0.9)
 model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])
 print(model.summary())
 return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
 # plot loss
 pyplot.subplot(211)
 pyplot.title('VGG 3 - Cross Entropy Loss')
 pyplot.plot(history.history['loss'], color='blue', label='train')
 pyplot.plot(history.history['val_loss'], color='orange', label='test')
 # plot accuracy
 pyplot.subplot(212)
 pyplot.title('VGG 3 - Classification Accuracy')
 pyplot.plot(history.history['accuracy'], color='blue', label='train')
 pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
 pyplot.show()
 
# run the test harness for evaluating a model
def run_test_harness():
 # define model
 model = VGG_3()
 # create data generator
 datagen = ImageDataGenerator(rescale=1.0/255.0)
 # prepare iterators
 train_it = datagen.flow_from_directory('/content/output/train',
 class_mode='binary', batch_size=64, target_size=(200, 200))
 test_it = datagen.flow_from_directory('/content/output/test',
 class_mode='binary', batch_size=64, target_size=(200, 200))

 t0 = time.time()
 # fit model
 history = model.fit(train_it, steps_per_epoch=len(train_it),
 validation_data=test_it, validation_steps=len(test_it), epochs=20, verbose=0)
 print("Training time:", time.time()-t0)

 #predict = model.predict(test_it, steps=len(test_it))

 # evaluate model
 _, acc = model.evaluate(test_it, steps=len(test_it), verbose=0)
 print('> %.3f' % (acc * 100.0))

 _, acc_t = model.evaluate(train_it, steps=len(train_it), verbose=0)
 print('> %.3f' % (acc_t * 100.0))

 # learning curves
 summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()

"""#VGG BLock 3 with Data Augmentation"""

# with augmentation

import sys
from matplotlib import pyplot
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator
import time
 
# define cnn model
def VGG_3_DA():
 model = Sequential()
 model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)))
 model.add(MaxPooling2D((2, 2)))
 model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
 model.add(MaxPooling2D((2, 2)))
 model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
 model.add(MaxPooling2D((2, 2)))
 model.add(Flatten())
 model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
 model.add(Dense(1, activation='sigmoid'))
 # compile model
 opt = SGD(lr=0.001, momentum=0.9)
 model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])
 print(model.summary())
 return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
 # plot loss
 pyplot.subplot(211)
 pyplot.title('Cross Entropy Loss')
 pyplot.plot(history.history['loss'], color='blue', label='train')
 pyplot.plot(history.history['val_loss'], color='orange', label='test')
 # plot accuracy
 pyplot.subplot(212)
 pyplot.title('Classification Accuracy')
 pyplot.plot(history.history['accuracy'], color='blue', label='train')
 pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
 pyplot.show()
 
# run the test harness for evaluating a model
def run_test_harness():
 # define model
 model = VGG_3_DA()

 # create data generators
 train_datagen = ImageDataGenerator(rescale=1.0/255.0,
 width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)
 test_datagen = ImageDataGenerator(rescale=1.0/255.0)

 # prepare iterators
 train_it = train_datagen.flow_from_directory('/content/output/train',
 class_mode='binary', batch_size=64, target_size=(200, 200))
 test_it = test_datagen.flow_from_directory('/content/output/test',
 class_mode='binary', batch_size=64, target_size=(200, 200))

 # fit model
 t0 = time.time()
 history = model.fit(train_it, steps_per_epoch=len(train_it),
 validation_data=test_it, validation_steps=len(test_it), epochs=50, verbose=0)
 print("Training time:", time.time()-t0, " s")

 # evaluate model
 _, acc = model.evaluate(test_it, steps=len(test_it), verbose=0)
 print('> %.3f' % (acc * 100.0))

 _, acc_t = model.evaluate(train_it, steps=len(train_it), verbose=0)
 print('> %.3f' % (acc_t * 100.0))
 
 # learning curves
 summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()

# import pandas as pd

# data = {'Training Time': [30],
#         'Trainng Loss': [90],
#         'Trainng Accuracy': [40],
#         'Testing Accuracy': [70],
#         'Number of Model Parameters': [70]
#         }

# df = pd.DataFrame(data)

# print(df)

"""Transfer learning using VGG16 or VGG19"""

# vgg16 model used for transfer learning on the dogs and cats dataset
import sys
from matplotlib import pyplot
from keras.utils import to_categorical
from keras.applications.vgg16 import VGG16
from keras.models import Model
from keras.layers import Dense
from keras.layers import Flatten
from keras.optimizers import SGD
from keras.preprocessing.image import ImageDataGenerator
import time
 
# define cnn model
def TL():
 # load model
 model = VGG16(include_top=False, input_shape=(224, 224, 3))
 # mark loaded layers as not trainable
 for layer in model.layers:
  layer.trainable = False
 # add new classifier layers
 flat1 = Flatten()(model.layers[-1].output)
 class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)
 output = Dense(1, activation='sigmoid')(class1)
 # define new model
 model = Model(inputs=model.inputs, outputs=output)
 # compile model
 opt = SGD(lr=0.001, momentum=0.9)
 model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])
 print(model.summary())
 return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
 # plot loss
 pyplot.subplot(211)
 pyplot.title('Cross Entropy Loss')
 pyplot.plot(history.history['loss'], color='blue', label='train')
 pyplot.plot(history.history['val_loss'], color='orange', label='test')
 # plot accuracy
 pyplot.subplot(212)
 pyplot.title('Classification Accuracy')
 pyplot.plot(history.history['accuracy'], color='blue', label='train')
 pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
 # save plot to file
 filename = sys.argv[0].split('/')[-1]
 pyplot.savefig(filename + '_plot.png')
 pyplot.show()
 
# run the test harness for evaluating a model
def run_test_harness():
 # define model
 model = TL()
 # create data generator
 datagen = ImageDataGenerator(featurewise_center=True)
 # specify imagenet mean values for centering
 datagen.mean = [123.68, 116.779, 103.939]
 # prepare iterator
 train_it = datagen.flow_from_directory('/content/output/train/',
 class_mode='binary', batch_size=64, target_size=(224, 224))
 test_it = datagen.flow_from_directory('/content/output/test',
 class_mode='binary', batch_size=64, target_size=(224, 224))
 # fit model
 to = time.time()
 history = model.fit_generator(train_it, steps_per_epoch=len(train_it),
 validation_data=test_it, validation_steps=len(test_it), epochs=10, verbose=1)
 print('Training time:', time.time()-to)
 # evaluate model
 _, acc = model.evaluate_generator(test_it, steps=len(test_it), verbose=0)
 print('> %.3f' % (acc * 100.0))

 _, acc1 = model.evaluate_generator(train_it, steps=len(train_it), verbose=0)
 print('> %.3f' % (acc1 * 100.0))
 # learning curves
 summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()

"""#TensorBoard"""

# !pip install tensorboard torch.utils.tensorboard

!unzip out.zip

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import torchvision
from torchvision.utils import make_grid
import matplotlib.pyplot as plt
import numpy as np


# Define hyperparameters
lr = 0.001
batch_size = 32
num_epochs = 10

# Define transforms for data preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
])

# Load data
train_dataset = datasets.ImageFolder(root='/content/output/train', transform=transform)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_dataset = datasets.ImageFolder(root='/content/output/test', transform=transform)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Define model
class VGG_1(nn.Module):
    def __init__(self):
        super(VGG_1, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(32 * 112 * 112, 128) # Update output size of fc1
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(128, 2)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.pool(x)
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x) 
        return x

model = VGG_1()

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=lr)

# Initialize TensorBoard writer
writer = SummaryWriter()

train_accuracy = []
predicted_images = []

# Train model
for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # Calculate training accuracy
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        accuracy = correct / batch_size
        train_accuracy.append(accuracy)

        # Append predicted images
        predicted_images.append(torchvision.utils.make_grid(images))

        # Write loss and accuracy to TensorBoard
        writer.add_scalar('Training loss', loss.item(), epoch * len(train_loader) + i)
        writer.add_scalar('Training accuracy', accuracy, epoch * len(train_loader) + i)

        # Evaluate model on test set after every 100 iterations
        if i % 100 == 0:
            correct = 0
            total = 0
            with torch.no_grad():
                for images, labels in test_loader:
                    outputs = model(images)
                    _, predicted = torch.max(outputs.data, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()

                    img_grid = torchvision.utils.make_grid(images)
                    writer.add_image('Test Images', img_grid, global_step=i)

                if total == 0:
                  accuracy = 0
                else:
                  accuracy = 100 * correct / total

                # Write accuracy to TensorBoard
                writer.add_scalar('Test accuracy', accuracy, epoch * len(train_loader) + i)
fig, ax = plt.subplots()
ax.plot(train_accuracy)
ax.set(xlabel='Training steps', ylabel='Training accuracy',
title='Training accuracy per step')
          
writer.add_figure('Training accuracy per step', fig)      

# Add predicted images to TensorBoard
stacked_images = torch.stack(predicted_images, dim=0)
writer.add_images('Predicted images', stacked_images, dataformats='NCHW')

# Write model to TensorBoard
writer.add_graph(model, images)

# Close TensorBoard writer
writer.close()

"""#Tensorboard VGG1"""

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir runs

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter

# Define hyperparameters
lr = 0.001
batch_size = 32
num_epochs = 10

# Define transforms for data preprocessing
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
])

# Load data
train_dataset = datasets.ImageFolder(root='/content/output/train', transform=transform)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_dataset = datasets.ImageFolder(root='/content/output/test', transform=transform)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Define model
class VGG_3(nn.Module):
    def __init__(self):
        super(VGG_3, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(128 * 28 * 28, 128) # Update input size of fc1
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(128, 2)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.pool(x)
        x = self.conv2(x)
        x = self.relu(x)
        x = self.pool(x)
        x = self.conv3(x)
        x = self.relu(x)
        x = self.pool(x)
        x = self.fc1(x.view(x.size(0), -1)) # Flatten the tensor before feeding it into the fully connected layer
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x

model = VGG_3()

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=lr)

# Initialize TensorBoard writer
writer = SummaryWriter()

train_accuracy = []
predicted_images = []

# Train model
for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # Calculate training accuracy
        _, predicted = torch.max(outputs.data, 1)
        correct = (predicted == labels).sum().item()
        accuracy = correct / batch_size
        train_accuracy.append(accuracy)

        # Append predicted images
        predicted_images.append(torchvision.utils.make_grid(images))

        # Write loss and accuracy to TensorBoard
        writer.add_scalar('Training loss', loss.item(), epoch * len(train_loader) + i)
        writer.add_scalar('Training accuracy', accuracy, epoch * len(train_loader) + i)

        # Evaluate model on test set after every 100 iterations
        if i % 100 == 0:
            correct = 0
            total = 0
            with torch.no_grad():
                for images, labels in test_loader:
                    outputs = model(images)
                    _, predicted = torch.max(outputs.data, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()

                    img_grid = torchvision.utils.make_grid(images)
                    writer.add_image('Test Images', img_grid, global_step=i)

                if total == 0:
                  accuracy = 0
                else:
                  accuracy = 100 * correct / total

                # Write accuracy to TensorBoard
                writer.add_scalar('Test accuracy', accuracy, epoch * len(train_loader) + i)
fig, ax = plt.subplots()
ax.plot(train_accuracy)
ax.set(xlabel='Training steps', ylabel='Training accuracy',
title='Training accuracy per step')
          
writer.add_figure('Training accuracy per step', fig)      

# Add predicted images to TensorBoard
stacked_images = torch.stack(predicted_images, dim=0)
writer.add_images('Predicted images', stacked_images, dataformats='NCHW')

# Write model to TensorBoard
writer.add_graph(model, images)

# Close TensorBoard writer
writer.close()

"""#VGG 3 Tensorboard"""

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir runs

